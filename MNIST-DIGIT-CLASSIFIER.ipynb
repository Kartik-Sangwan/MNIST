{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# These are all the libraries that you must downlaod to run the code.\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening up our mnist kaggle trainnig dataset\n",
    "# Locate the train.csv file in your computer and enter its path here.\n",
    "data = pd.read_csv('/Users/kartiksangwan/Desktop/ML/Mnist-Data-Set/train.csv')\n",
    "dummies = pd.get_dummies(data['label'])\n",
    "dummies.head(10)\n",
    "dummies[2:3:].values.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow-sessions/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAGfCAYAAAAd79YcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEQlJREFUeJzt3W+opnWdx/HPNyef9McMUcTc1RVZdhM0GWTDWhQz3J6oRNEUMhvBFCQULLFShD4Rcql2HwSBojRGKoKlJsv6D8EWt/IPkn9mSwnXzEEJoaYHpTm/fTC3NGPz59zX3Od855z79YLhnHOd+3d+Py6umfdc13XPNTXGCAB0eUv3AgBYbkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFab1nKyqvIYB4AlMcaolbzOGREArYQIgFaHFaKquqiqfl5Vz1bVFYtaFADLo6Y+fbuqjkryiyQXJnkhycNJtowxnj7IGPeIAJbEWtwjOifJs2OMX44xXk1yS5KLD+PnAbCEDidEJyX51V5fvzDbBgArdjhv397fKddfXHqrqm1Jth3GPABsYIcToheSnLzX1+9J8uKbXzTGuDbJtYl7RAD8pcO5NPdwktOr6tSqOjrJJ5LcuZhlAbAsJp8RjTH+VFWXJ7k7yVFJbhhjPLWwlQGwFCa/fXvSZC7NASwNj/gBYF0QIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGglRAC0EiIAWgkRAK2ECIBWQgRAKyECoJUQAdBKiABoJUQAtBIiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGglRAC0EiIAWgkRAK2ECIBWQgRAKyECoJUQAdBKiABoJUQAtBIiAFpt6l4AwErcd999k8ZdcMEFc4/ZunXrpLluvPHGSeOWnTMiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKgladvA2vugQcemHvMueeeO2mu3bt3zz1mjDFpLqZxRgRAKyECoNVhXZqrqueS7EryepI/jTE2L2JRACyPRdwjOn+M8ZsF/BwAlpBLcwC0OtwQjST3VNWjVbVtEQsCYLkc7qW5c8cYL1bV8Unurar/HWM8uPcLZoESKQD267DOiMYYL84+vpzkB0nO2c9rrh1jbPZGBgD2Z3KIquptVfWONz5P8uEkTy5qYQAsh8O5NHdCkh9U1Rs/56Yxxn8tZFUALI3JIRpj/DLJmQtcCwBLyNu3AWglRAC08vRtYLKvfOUrk8a9//3vn3vMUUcdNWmuW2+9de4xt91226S5mMYZEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKgVY0x1m6yqrWbDFixSy65ZNK4m2++edK4o48+eu4xTzzxxKS5PvjBD849ZteuXZPmYl9jjFrJ65wRAdBKiABoJUQAtBIiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArTZ1LwBYrJNPPnnuMVdeeeWkuaY8RTtJXnnllbnHfPWrX500lydpH/mcEQHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGglRAC0EiIAWtUYY+0mq1q7yWCdO+eccyaNu+666+Yec8YZZ0yaa6pPfepTc4+55ZZbVmElrKYxRq3kdc6IAGglRAC0EiIAWgkRAK2ECIBWQgRAKyECoJUQAdBKiABoJUQAtBIiAFoJEQCthAiAVpu6FwAb3WWXXTZp3Pbt2yeNm/JE/d/+9reT5rrvvvsmjbv77rsnjWNjckYEQCshAqDVIUNUVTdU1ctV9eRe295dVfdW1TOzj8eu7jIB2KhWckb0nSQXvWnbFUnuH2OcnuT+2dcAMLdDhmiM8WCSV960+eIkb9xJ3Z7kkgWvC4AlMfUe0QljjJ1JMvt4/OKWBMAyWfW3b1fVtiTbVnseANanqWdEL1XViUky+/jygV44xrh2jLF5jLF54lwAbGBTQ3Rnkq2zz7cmuWMxywFg2azk7ds3J/mfJH9bVS9U1WeSfC3JhVX1TJILZ18DwNwOeY9ojLHlAN+6YMFrAWAJebICAK2ECIBWnr4NczjhhBPmHvOlL31pFVayWHfcMe39Rp/+9KcXvBKWkTMiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArDz1lKb3rXe+aNO6ee+6Ze8x73/veSXNNtWvXrrnH3HnnnauwElgZZ0QAtBIiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBECrGmOs3WRVazcZHMRJJ500adzzzz+/4JUcWFVNGnfMMcfMPWbKE7vhUMYYKzqInREB0EqIAGglRAC0EiIAWgkRAK2ECIBWQgRAKyECoJUQAdBKiABoJUQAtBIiAFpt6l4AHK7jjjtu7jE//OEPJ8019UGkU/z4xz+eNO7VV19d8EpgdTkjAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGglRAC0EiIAWnn6Nuvet771rbnHnHnmmZPmGmPMPeahhx6aNNeHPvShSeP++Mc/ThoHXZwRAdBKiABodcgQVdUNVfVyVT2517arqurXVfX47NdHVneZAGxUKzkj+k6Si/az/d/HGGfNfv3nYpcFwLI4ZIjGGA8meWUN1gLAEjqce0SXV9XPZpfujl3YigBYKlND9O0kpyU5K8nOJN840AuraltVPVJVj0ycC4ANbFKIxhgvjTFeH2PsTnJdknMO8tprxxibxxibpy4SgI1rUoiq6sS9vrw0yZMHei0AHMwhn6xQVTcnOS/JcVX1QpIrk5xXVWclGUmeS/LZVVwjABvYIUM0xtiyn83Xr8JaAFhCnqwAQCshAqCVp29zxDjuuOMmjTvttNMWvJIDe+211+Yec80110yay1O0WRbOiABoJUQAtBIiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArTz0lIU7/vjjJ4276aabJo07++yz5x7zhz/8YdJcn/vc5+Yec9ddd02aC5aFMyIAWgkRAK2ECIBWQgRAKyECoJUQAdBKiABoJUQAtBIiAFoJEQCthAiAVkIEQCshAqCVp2+zcJdeeumkceeff/6CV3JgP/3pTyeN++53v7vglQDOiABoJUQAtBIiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFaevs1BbdmyZe4x11xzzSqs5MAeeuihucd88pOfXIWVAFM4IwKglRAB0EqIAGglRAC0EiIAWgkRAK2ECIBWQgRAKyECoJUQAdBKiABoJUQAtKoxxtpNVrV2k7GPY445ZtK4Rx99dO4xp5566qS5pvroRz8695jbb799FVYC7G2MUSt5nTMiAFoJEQCtDhmiqjq5qh6oqh1V9VRVfWG2/d1VdW9VPTP7eOzqLxeAjWYlZ0R/SvIvY4y/S/IPST5fVX+f5Iok948xTk9y/+xrAJjLIUM0xtg5xnhs9vmuJDuSnJTk4iTbZy/bnuSS1VokABvXXPeIquqUJO9L8pMkJ4wxdiZ7YpXk+EUvDoCNb9NKX1hVb09yW5IvjjF+V7Wid+WlqrYl2TZteQBsdCs6I6qqt2ZPhL43xvj+bPNLVXXi7PsnJnl5f2PHGNeOMTaPMTYvYsEAbCwreddcJbk+yY4xxjf3+tadSbbOPt+a5I7FLw+AjW4ll+bOTXJZkieq6vHZti8n+VqSW6vqM0meT/Kx1VkiABvZIUM0xvjvJAe6IXTBYpcDwLLxZAUAWgkRAK1W/PZt1reLL7540ri1fpL2FO985zu7lwAcBmdEALQSIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGglRAC0EiIAWgkRAK2ECIBWHnq6JF577bVJ43bv3j33mLe8Zdrfb15//fVJ404//fRJ44AjgzMiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKgVY0x1m6yqrWbjIV4+umn5x6zadO0h7pfffXVk8Zt37590jhgdY0xaiWvc0YEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGglRAC08vRtAFaFp28DsC4IEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGh1yBBV1clV9UBV7aiqp6rqC7PtV1XVr6vq8dmvj6z+cgHYaA75H+NV1YlJThxjPFZV70jyaJJLknw8ye/HGF9f8WT+YzyApbHS/xhv0wp+0M4kO2ef76qqHUlOOrzlAcAec90jqqpTkrwvyU9mmy6vqp9V1Q1VdeyC1wbAElhxiKrq7UluS/LFMcbvknw7yWlJzsqeM6ZvHGDctqp6pKoeWcB6AdhgDnmPKEmq6q1J7kpy9xjjm/v5/ilJ7hpjnHGIn+MeEcCSWOk9opW8a66SXJ9kx94Rmr2J4Q2XJnly3kUCwEreNfeBJD9K8kSS3bPNX06yJXsuy40kzyX57OyNDQf7Wc6IAJbESs+IVnRpblGECGB5LOzSHACsJiECoJUQAdBKiABoJUQAtBIiAFoJEQCthAiAVkIEQCshAqCVEAHQSogAaCVEALQSIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGglRAC0EiIAWgkRAK2ECIBWQgRAKyECoJUQAdBKiABoJUQAtNq0xvP9Jsn/HeB7x82+zx72x77sj33ZH/uyP/7sSNkXf73SF9YYYzUXsmJV9cgYY3P3Oo4U9se+7I992R/7sj/+bD3uC5fmAGglRAC0OpJCdG33Ao4w9se+7I992R/7sj/+bN3tiyPmHhEAy+lIOiMCYAm1h6iqLqqqn1fVs1V1Rfd6ulXVc1X1RFU9XlWPdK9nrVXVDVX1clU9ude2d1fVvVX1zOzjsZ1rXEsH2B9XVdWvZ8fI41X1kc41rqWqOrmqHqiqHVX1VFV9YbZ9KY+Rg+yPdXWMtF6aq6qjkvwiyYVJXkjycJItY4yn2xbVrKqeS7J5jHEk/DuANVdV/5jk90luHGOcMdv2b0leGWN8bfaXlWPHGP/auc61coD9cVWS348xvt65tg5VdWKSE8cYj1XVO5I8muSSJP+cJTxGDrI/Pp51dIx0nxGdk+TZMcYvxxivJrklycXNa6LRGOPBJK+8afPFSbbPPt+ePb/RlsIB9sfSGmPsHGM8Nvt8V5IdSU7Kkh4jB9kf60p3iE5K8qu9vn4h63AnLthIck9VPVpV27oXc4Q4YYyxM9nzGy/J8c3rORJcXlU/m126W4rLUG9WVackeV+Sn8Qx8ub9kayjY6Q7RLWfbcv+Nr5zxxhnJ/mnJJ+fXZqBvX07yWlJzkqyM8k3epez9qrq7UluS/LFMcbvutfTbT/7Y10dI90heiHJyXt9/Z4kLzat5Ygwxnhx9vHlJD/InsuXy+6l2bXwN66Jv9y8nlZjjJfGGK+PMXYnuS5LdoxU1Vuz5w/d740xvj/bvLTHyP72x3o7RrpD9HCS06vq1Ko6OsknktzZvKY2VfW22Q3HVNXbknw4yZMHH7UU7kyydfb51iR3NK6l3Rt/4M5cmiU6RqqqklyfZMcY45t7fWspj5ED7Y/1doy0/4PW2dsK/yPJUUluGGNc3bqgRlX1N9lzFpTseTL6Tcu2P6rq5iTnZc8ThF9KcmWS25PcmuSvkjyf5GNjjKW4gX+A/XFe9lxyGUmeS/LZN+6PbHRV9YEkP0ryRJLds81fzp77Ikt3jBxkf2zJOjpG2kMEwHLrvjQHwJITIgBaCREArYQIgFZCBEArIQKglRAB0EqIAGj1/3XOsTyo2c43AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#y are the lables\n",
    "Y = data['label'].values\n",
    "\n",
    "#x is training dataset\n",
    "\n",
    "x = data.drop('label', axis = 1)\n",
    "X = x.values\n",
    "\n",
    "#visualisation of the data\n",
    "i = 0\n",
    "\n",
    "# Basic graphing using matplotlib functions.\n",
    "plt.figure(figsize = (7,7))\n",
    "grid_data = x.iloc[i].as_matrix().reshape(28, 28)\n",
    "plt.imshow(grid_data, interpolation = 'none', cmap = 'gray')\n",
    "plt.show()\n",
    "print(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "# I have used the most basic activation function called Sigmoid and its just a mathematical\n",
    "# formula that converts any number into a number between 0 and 1 so we can clasiify if the number \n",
    "# is say 7 or not. It can be thought of a probablilty that the input is say 7.\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_p(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Learning rate can be played around with.\n",
    "# It is basically how fast or slow the NN takes its steps towards the required minima.\n",
    "learning_rate = 0.0003\n",
    "\n",
    "# In this function we define the architecture of the the Neural Network.\n",
    "# We can change the number of neurons, the number of innner layers and see the \n",
    "# effect on the performance of the NN.\n",
    "def initialise_par(nninput_dim = 784, nnhidden_dim_1 = 256, nnhidden_dim_2 = 256, nnoutput_dim = 10):\n",
    "    #initialising the parameters\n",
    "    w1 = np.random.randn(nninput_dim, nnhidden_dim_1) - 1\n",
    "    w2 = np.random.randn(nnhidden_dim_1, nnhidden_dim_2) - 1\n",
    "    w3 = np.random.randn(nnhidden_dim_2, nnoutput_dim) - 1\n",
    "    b1 = np.zeros((1, nnhidden_dim_1)) \n",
    "    b2 = np.zeros((1, nnhidden_dim_2)) \n",
    "    b3 = np.zeros((1, nnoutput_dim)) \n",
    "    #return the model parameters\n",
    "    model = {'w1': w1, 'w2': w2, 'w3': w3, 'b1':b1, 'b2':b2, 'b3':b3}\n",
    "    return model\n",
    "def forward_prop(model, a):\n",
    "    #get parameters of model\n",
    "    w1, w2, w3, b1, b2, b3 = model['w1'], model['w2'], model['w3'], model['b1'], model['b2'], model['b3']\n",
    "    #forward prop to produce predicted value\n",
    "    z1 = np.dot(a, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    preds = a3\n",
    "    #cache = {'a1':a1, 'a2': a2, 'a3':a3, 'z1':z1, 'z2':z2, 'z3':z3}\n",
    "    #return the 1*10 array of predictions\n",
    "    return preds\n",
    "def next_batch(X, Y, batch_size):\n",
    "    # loop over our dataset `X` in mini-batches of size `batchSize`\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (X[i:i + batch_size], Y[i:i + batch_size])\n",
    "\n",
    "loss_history = []\n",
    "# This is the main train method and it parameters can be changed to increase the accuracy \n",
    "# of the NN. For e.g. a increase in the size of the training data has an direct effect on the \n",
    "# the final accuracy of the NN.\n",
    "# Number of iteration (i.e. epochs) is another major parameter influencing the NN performance.\n",
    "# The batch size can be increased or decreased to speedup/slowdown the NN thus enchancing performance.\n",
    "# The initial biases for the neurons can also be set in the initialise_par function.\n",
    "# Over all the NN is highly customisable and has great scope for further enhancements.\n",
    "def train(model, X, Y):\n",
    "    w1, w2, w3, b1, b2, b3 = model['w1'], model['w2'], model['w3'], model['b1'], model['b2'], model['b3']\n",
    "    for epoch in range(200):\n",
    "        epoch_loss = []\n",
    "        for (batchx, batchy) in next_batch(X[:30000:], Y, 100):\n",
    "            #forward prop to produce predicted value\n",
    "            z1 = np.dot(batchx, w1) + b1\n",
    "            a1 = sigmoid(z1)\n",
    "            z2 = np.dot(a1, w2) + b2\n",
    "            a2 = sigmoid(z2)\n",
    "            z3 = np.dot(a2, w3) + b3\n",
    "            a3 = sigmoid(z3)\n",
    "            preds = a3\n",
    "            #calculating error \n",
    "            error = preds - batchy\n",
    "            delta_3 = error\n",
    "            loss = np.sum(error ** 2)\n",
    "            epoch_loss.append(loss)\n",
    "            #back propogation of the error(cost) to prevoius layers using formulas derived by differentiating the cost \n",
    "            #funtion wrt inputs to specific layers and also wrt to weights and the biases as shown below\n",
    "            delta_2 = np.multiply(np.dot(delta_3, w3.T), sigmoid_p(a2))\n",
    "            delta_1 = np.multiply(np.dot(delta_2, w2.T), sigmoid_p(a1))\n",
    "            #calculating the rate of change of the cost function wrt the weights(for which we need the above deltas)\n",
    "            dw3 = np.dot(a2.T, delta_3)\n",
    "            dw2 = np.dot(a1.T, delta_2)\n",
    "            dw1 = np.dot(batchx.T, delta_1)\n",
    "            #calculating the rate of change of the cost function wrt the biases(for which we need the above deltas)\n",
    "            db1 = 1/200 * np.sum(delta_1, axis=0)\n",
    "            db2 = 1/200 * np.sum(delta_2, axis=0)\n",
    "            db3 = 1/200 * np.sum(delta_3, axis=0)\n",
    "  \n",
    "            db3.values.reshape(1, 10)\n",
    "            #updating our weights using gradient descend\n",
    "            w1 -= learning_rate * dw1\n",
    "            w2 -= learning_rate * dw2\n",
    "            w3 -= learning_rate * dw3\n",
    "            b1 -= learning_rate * db1\n",
    "            b2 -= learning_rate * db2\n",
    "     \n",
    "            #b3 -= learning_rate * db3\n",
    "        loss_history.append(np.average(epoch_loss))\n",
    "        model = {'w1': w1, 'w2': w2, 'w3': w3, 'b1':b1, 'b2':b2, 'b3':b3}    \n",
    "        if epoch % 20 == 0:\n",
    "            print(loss_history)\n",
    "    return model\n",
    "def predict(model, x):\n",
    "    prediction = forward_prop(model, x)\n",
    "    loc_max = prediction.argmax()\n",
    "    return loc_max\n",
    "\n",
    "def cal_accuracy(m, dummies, X):\n",
    "    correct_guess = 0\n",
    "    for i in range(5000):\n",
    "        a0 = X[i+30000:i+30001:]\n",
    "        number_guess = predict(m, a0)\n",
    "        number = dummies[i+30000:i+30001:].values.argmax()\n",
    "        if number == number_guess:\n",
    "            correct_guess += 1\n",
    "    return correct_guess/50\n",
    "        \n",
    "m = initialise_par()\n",
    "final_model_par = train(m, X, dummies)\n",
    "# visual representation of our loss function.\n",
    "# Clearly it is sloping downwards and hence our NN is learnig properly and will give a good accuracy.\n",
    "plt.plot(loss_history)\n",
    "# This accuracy has been calculated using the images that were never seen by the NN.\n",
    "cal_accuracy(final_model_par, dummies, X)\n",
    "\n",
    "# This can be used to predict the number in any image that the neural network has not prevoiusly seen.\n",
    "predict(m, 35000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
